[
  {
    "objectID": "Bayes.html",
    "href": "Bayes.html",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "#### Independent Monte Carlo ####\n## Data\nn <- 20\nsumy <- 40.4\nsumysq <- 93.2\nR <- 5e03\n## Posterior \nbetan <- sumy / (n + 1)\nan <- 2 + n / 2\nbn <- 2 + 0.5 * (sumysq - ((sumy ^ 2)) / (n + 1))\n## Independent Monte Carlo\nset.seed(12345)\nsigma.sq <- rep(NA, R)\nbeta <- rep(NA, R)\nfor(r in 1 : R){\n    sigma.sq[r] <- 1 / rgamma(n = 1, shape = an, rate = bn)\n    beta[r] <- rnorm(n = 1, mean = betan, sd = sqrt(sigma.sq[r] / (n + 1)))\n}\n## Plot the results\npar(mar = c(4.1, 4.1, 1.5, 1))\nplot(cumsum(beta) / (1 : R), type = \"l\", xlab = \"n\", ylab = \"Independent Monte Carlo approximation\")\nabline(h = sumy / (n + 1), col = 2, lty = 2, lwd = 2)\n\n\n\n\n\n\n\n\n#### Importance Sampling ####\n## Data\nn <- 20\nsumy <- 40.4\nybar <- sumy / 20\nsumysq <- 93.2\nysd <- sqrt((sumysq - n * ybar ^ 2) / (n - 1))\n## Posterior \nbetan <- sumy / (n + 1)\nan <- 2 + n / 2\nbn <- 2 + 0.5 * (sumysq - ((sumy ^ 2)) / (n + 1))\n## Density of inverse-Gamma distribution\nIvGamma <- function(x, a, b){\n    exp(a * log(b) - lgamma(a) - (a + 1) * log(x) - b / x)\n} \n## Monte Carlo\nset.seed(123456)\nR <- 5e03\nh.pi.g <- rep(NA, R)\nfor(r in 1 : R){\n    ## We use Exp(1 / ybar) as the importance distribution\n    proposal <- rexp(n = 1, rate = 1 / ybar)\n    ## Compute the value of h(theta) * pi(theta | data) / g(theta)\n    h.pi.g[r] <- proposal * IvGamma(proposal, an, bn) / dexp(proposal, rate = 1 / ybar)\n}\n## Plot the results\npar(mar = c(4.1, 4.1, 1.5, 1))\nplot(cumsum(h.pi.g) / (1 : R), type = \"l\", xlab = \"n\", ylab = \"Importance Sampling\")\nabline(h = bn / (an - 1), col = 2, lty = 2, lwd = 2)\n\n\n\n\n\n\n\n\n#### Normalized Importance Sampling ####\nset.seed(123456)\nw <- rep(NA, R)\nNIS <- rep(NA, R)\nfor(r in 1 : R){\n    ## Simulate sigma and beta\n    sigma.sq <- rexp(n = 1, rate = 1 / ybar)\n    beta <- rnorm(n = 1, mean = ybar, sd = ysd)\n    ## Compute the densities\n    posterior <- exp(-0.5 * ((n + 1) * beta ^ 2 - 2 * beta * sumy + 4 + sumysq) / sigma.sq) / (sigma.sq ^ (0.5 * (n + 1) + 3))\n    g.density <- dnorm(x = beta, mean = ybar, sd = ysd) * dexp(x = sigma.sq, rate = 1 / ybar)\n    ## normalized IS estimator\n    w[r] <- posterior / g.density\n    NIS[r] <- w[r] * sigma.sq\n}\n## Plot the results\npar(mar = c(4.1, 4.1, 1.5, 1))\nplot(cumsum(NIS) / cumsum(w), type = \"l\", xlab = \"n\", ylab = \"Normalized Importance Sampling\")\nabline(h = bn / (an - 1), col = 2, lty = 2, lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n#### Metropolis-Hastings Algorithm ####\n## Data\nn <- 20\nsumy <- 40.4\nsumysq <- 93.2\nybar <- sumy / n\nysd <- sqrt((sumysq - n * ybar ^ 2) / (n - 1))\n## Define function for posterior density\nposterior <- function(beta, sigma.sq) {\n    exp(-0.5 * ((n + 1) * beta ^ 2 - 2 * beta * sumy + 4 + sumysq) / sigma.sq) / \n        (sigma.sq ^ (0.5 * (n + 1) + 3))\n}\n## Define function for Metropolis-Hastings algorithm\nMHalgorithm <- function(L, initial, sd) {   \n    # L is the length of the Markov chain, including the burn-in period\n    # initial is the initial state\n    # sd is the standard deviation of the proposal distribution for beta\n    chain <- rbind(c(initial), matrix(NA, L, length(initial)))\n    for(t in 1 : L){\n        # Propose a candidate\n        beta.prop <- rnorm(n = 1, mean = chain[t, 1], sd = sd)\n        sigmasq.prop <- rexp(n = 1, rate = 1 / chain[t, 2])\n        # Calculate the ratio\n        numerator <- posterior(beta = beta.prop, sigma.sq = sigmasq.prop) * \n            dnorm(x = chain[t, 1], mean = beta.prop, sd = sd) * \n            dexp(x = chain[t, 2], rate = 1 / sigmasq.prop)\n        denominator <- posterior(beta = chain[t, 1], sigma.sq = chain[t, 2]) * \n            dnorm(x = beta.prop, mean = chain[t, 1], sd = sd) * \n            dexp(x = sigmasq.prop, rate = 1 / chain[t, 2])\n        r <- numerator / denominator\n        # Generate U(0, 1)\n        u <- runif(1, 0, 1)\n        # Update\n        if(u <= r) {     \n            chain[t + 1, ] <- c(beta.prop, sigmasq.prop)\n        } else {     \n            chain[t + 1, ] <- chain[t, ]       \n        }\n    }\n    chain\n}\n## Perform MCMC sampling\nset.seed(12345)\nMH <- MHalgorithm(L = 20000, initial = c(0, 1), sd = 0.2) # sd is a tuning parameter\npar(mfrow = c(2, 1), mar = c(4.1, 4.1, 1.5, 1)) \nplot(MH[, 1], type = \"l\", ylab = expression(beta))  \nplot(MH[, 2], type = \"l\", ylab = expression(sigma^2))\n\n\n\n\n\n\n\n\n#### Gibbs Sampler ####\n## Generate some fake data\nset.seed(12345)\nMu <- rnorm(1, 0, 2) \nLambda <- rexp(1, rate = 2) \nData <- rnorm(500, mean = Mu, sd = 1 / sqrt(Lambda)) \n## Define function for Gibbs sampler\nGibbs <- function(L, initial, data, mu0, lam0, b0){    \n    # L is the length of the Markov chain, including the burn-in period\n    # initial is the initial state\n    # data is our data\n    # mu0, lam0 and b0 are the hyperparameters\n    \n    ## Information from data\n    n <- length(data)     \n    s <- sum(data)     \n    s2 <- sum(data ^ 2)   \n    ## Initiate mu and lambda\n    mu <- numeric(1 + L)     \n    mu[1] <- initial[1]     \n    lambda <- numeric(1 + L)  \n    lambda[1] <- initial[2] \n    ## Run MCMC in a for loop\n    for(t in 2 : (1 + L)){         \n        ## Generate mu given lambda\n        mean_norm <- (s / lam0 + mu0 / lambda[t - 1]) / (n / lam0 + 1 / lambda[t - 1])         \n        sd_norm <- 1 / sqrt(lambda[t - 1] * lam0 * (n / lam0 + 1 / lambda[t - 1]))         \n        mu[t] <- rnorm(n = 1, mean = mean_norm, sd = sd_norm)  \n        ## Generate lambda given mu\n        lambda[t] <- rgamma(n = 1, shape = n / 2 + 1, \n                            rate = b0 + 0.5 * (s2 - 2 * mu[t] * s + n * mu[t] ^ 2))     \n    }     \n    ## Return the chain \n    chain <- cbind(mu, lambda)     \n    colnames(chain) <- c(\"mu\", \"lambda\")     \n    return(chain) \n} \n## Run Gibbs sampler\nset.seed(123456) \nGibbsEx <- Gibbs(L = 10000, initial = c(0, 1), data = Data, mu0 = 1, lam0 = 1, b0 = 1) \n## Plot results\npar(mfrow = c(2, 1), mar = c(4.1, 4.1, 1, 1)) \nplot(GibbsEx[, \"mu\"], type = \"l\", ylab = expression(mu)) \nplot(GibbsEx[, \"lambda\"], type = \"l\", ylab = expression(lambda)) \n\n\n\n\n\n\n\n\n#### Hamiltonian MC ####\n## log posterior density and its gradient\nlogpi <- function(beta, sigma.sq, n, sumy, sumysq) {\n    -0.5 * ((n + 1) * beta ^ 2 - 2 * beta * sumy + 4 + sumysq) / sigma.sq - (0.5 * (n + 1) + 3) * log(sigma.sq)\n}\ngrad_logpi <- function(beta, sigma.sq, n, sumy, sumysq){\n    der_beta <- -((n + 1) * beta - sumy) / sigma.sq\n    der_sigma2 <- 0.5 * ((n + 1) * beta ^ 2 - 2 * beta * sumy + 4 + sumysq) / (sigma.sq ^ 2) - (0.5 * (n + 1) + 3) / sigma.sq\n    return(matrix(c(der_beta, der_sigma2), nrow = 2, ncol = 1))\n}\nf <- function(x) logpi(beta = x[1], sigma.sq = x[2], n = 20, sumy = 40.4, sumysq = 93.2)\nnumDeriv::grad(func = f, x = c(0.1, 0.3))\n\n[1] 127.6667 451.2778\n\ngrad_logpi(beta = 0.1, sigma.sq = 0.3, n = 20, sumy = 40.4, sumysq = 93.2)\n\n         [,1]\n[1,] 127.6667\n[2,] 451.2778\n\n## Define function for HMC\nHMC <- function(n, x0, n.obs, sumy, sumysq, logpi, grad_logpi, sd_phi, epsilon, L) {     \n    # n is the length of the Markov chain, including burn-in period     \n    # x0 is the initial state\n    # n.obs is the number of observations\n    # logpi is the function to calculate the posterior\n    # grad_logpi is the function to calculate the gradient of the posterior\n    # sd_phi is the vector of standard deviation used to generate the momentum from a normal distribution with zero mean\n    # epsilon is the step size in Leapfrog method\n    # L is the number of steps in Leapfrog method\n    \n    ## Initiate the Markov chain\n    chain <- rbind(x0, matrix(NA, n, 2))     \n    ## Generate random number using HMC in a for loop\n    for(t in 1 : n){         \n        ## Draw momentum from normal         \n        phi0 <- c(rnorm(1, mean = 0, sd = sd_phi[1]), rnorm(1, mean = 0, sd = sd_phi[2]))     \n        #-------------------------------------#         \n        # Leapfrog to update x         \n        x <- chain[t, ]         \n        # Current Hamiltonian         \n        H <- -logpi(beta = x[1], sigma.sq = x[2], n = n.obs, sumy, sumysq) - dnorm(phi0[1], mean = 0, sd = sd_phi[1], log = TRUE) - dnorm(phi0[2], mean = 0, sd = sd_phi[2], log = TRUE)         \n        # Make a half step for momentum at the beginning         \n        phi <- phi0 + epsilon * grad_logpi(beta = x[1], sigma.sq = x[2], n = n.obs, sumy, sumysq) / 2        \n        # Alternate full steps for position and momentum         \n        for (i in 1 : L){             \n            # Make a full step for the position             \n            x <- x + epsilon * diag(1 / sd_phi, 2) %*% phi             \n            # Make a full step for the momentum, except at end of trajectory             \n            if (i != L) {                 \n                phi <- phi + epsilon * grad_logpi(beta = x[1], sigma.sq = x[2], n = n.obs, sumy, sumysq)            \n            }         \n        }         \n        # Make a half step for momentum at the end.         \n        phi <- phi + epsilon * grad_logpi(beta = x[1], sigma.sq = x[2], n = n.obs, sumy, sumysq) / 2         \n        #-------------------------------------#         \n        # Negate momentum at end of trajectory to make the proposal symmetric         \n        phistar <- -phi         \n        # Proposed Hamiltonian         \n        Hstar <- -logpi(beta = x[1], sigma.sq = x[2], n = n.obs, sumy, sumysq) - dnorm(phistar[1], mean = 0, sd = sd_phi[1], log = TRUE) - dnorm(phistar[2], mean = 0, sd = sd_phi[2], log = TRUE)               \n        # Metropolis ratio         \n        ratio <- exp(H - Hstar)         \n        # Accept or reject the state at end of trajectory, returning either         \n        # the position at the end of the trajectory or the initial position         \n        if (runif(1) < ratio){             \n            chain[t + 1, ] <- x         \n        } else {             \n            chain[t + 1, ] <- chain[t, ]          \n        }     \n    }     \n    return(chain)\n}\n## Run HMC\nset.seed(12345)\nHMCdraw <- HMC(n = 10000, x0 = c(0, 0.6), n.obs = 20, sumy = 40.4, sumysq = 93.2,\n               logpi = logpi, grad_logpi = grad_logpi,                 \n               sd_phi = c(1.5, 1.5), epsilon = 0.05, L = 20) \n## Plot results\npar(mfrow = c(2, 1), mar = c(4.1, 4.1, 1, 1)) \nplot(HMCdraw[-c(1 : 5000), 1], type = \"l\")  \nplot(HMCdraw[-c(1 : 5000), 2], type = \"l\") \n\n\n\n\n\n\n\n\n\n## Our data\n## Generate some fake data\nset.seed(12345)\nMu <- rnorm(1, 0, 2) \nLambda <- rexp(1, rate = 2) \nN <- 500\nData <- rnorm(N, mean = Mu, sd = 1 / sqrt(Lambda)) \n## Define function for Gibbs sampler\nGibbs <- function(L, initial, data, mu0, lam0, a0, b0){    \n    # L is the length of the Markov chain, including the burn-in period\n    # initial is the initial state\n    # data is our data\n    # mu0, lam0, a0, and b0 are the hyperparameters\n    \n    ## Information from data\n    n <- length(data)     \n    sumx <- sum(data)     \n    sumx2 <- sum(data ^ 2)   \n    ## Initiate mu and lambda\n    mu <- numeric(1 + L)     \n    mu[1] <- initial[1]     \n    lambda <- numeric(1 + L)  \n    lambda[1] <- initial[2] \n    ## Run MCMC in a for loop\n    for(t in 2 : (1 + L)){       \n        ## Generate mu given lambda\n        normal.mean <- (lam0 * mu0 + lambda[t - 1] * sumx) / (lam0 + n * lambda[t - 1])\n        normal.var <- lam0 + n * lambda[t - 1]\n        mu[t] <- rnorm(1, normal.mean, 1 / sqrt(normal.var)) \n        ## Generate lambda given mu\n        bn <- b0 + 0.5 * sumx2 - sumx * mu[5] + 0.5 * n * mu[5]^2       \n        lambda[t] <- rgamma(n = 1, shape = a0 + n / 2, rate = bn)     \n    }     \n    ## Return the chain \n    chain <- cbind(mu, lambda)     \n    colnames(chain) <- c(\"mu\", \"lambda\")     \n    return(chain) \n} \n## Run Gibbs sampler multiple times\na0 <- 1\nb0 <- 1\nRep <- 1e03 \nNaive <- RB <- numeric(Rep) \nfor(r in 1 : Rep){     \n    MuLambda <- Gibbs(L = 1000, initial = c(0, 1), data = Data, mu0 = 1, lam0 = 1, a0 = a0, b0 = b0)\n    ## Discard burn-in period\n    MuLambda <- MuLambda[-c(1 : 500), ]\n    ## Naive MC     \n    Naive[r] <- mean(MuLambda[, 2])     \n    ## Rao-Blackwell     \n    Bn <- b0 + 0.5 * sum(Data ^ 2)  - sum(Data) * MuLambda[, 1] + 0.5 * N * (MuLambda[, 1] ^ 2)\n    RB[r] <- mean((a0 + N / 2) / Bn) \n} \n## Compare results with and without Rao-Blackwell\nmean(Naive); var(Naive) \n\n[1] 0.289172\n\n\n[1] 1.992623e-06\n\nmean(RB); var(RB)\n\n[1] 0.289427\n\n\n[1] 1.290195e-09\n\n\n\n\n\n\n\n\nSuppose that \\(X_{i}\\mid\\theta\\sim\\text{Bernoulli}\\left(\\theta\\right)\\) and \\(\\theta\\sim\\text{Beta}\\left(a_{0},b_{0}\\right)\\). We load data\n\nload(\"Bernoulli.RData\")\n\n\n## Step 1: create Stan program \n#### It has three blocks: data, parameters, model \nbetabinomial =  \" \ndata {   \n   int<lower=0> N ; // Number of Bernoulli variables   \n   int<lower=0, upper=1> y[N] ; // integer valued y of length N  \n}\nparameters {   \n   real<lower=0, upper=1> theta ; // success probability \n}\nmodel {   \n   theta ~ uniform(0, 1) ; // prior   \n   y ~ bernoulli(theta) ; // likelihood \n} \n\"\n\n## Step 2: Posterior simulation using Stan \n## It takes time the first time, because the code needs to be compiled first.  \nlibrary(rstan) \nNUTS <- stan(model_code = betabinomial, data = list(y = Success, N = 20),              \n            iter = 5000, # the length of each Markov chain, including warmup             \n            warmup = 5000 / 2, # default is 50% is burn-in              \n            thin = 1, # 1 means that no thining is done\n            chains = 4)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 5000 [  0%]  (Warmup)\nChain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)\nChain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)\nChain 1: Iteration: 1500 / 5000 [ 30%]  (Warmup)\nChain 1: Iteration: 2000 / 5000 [ 40%]  (Warmup)\nChain 1: Iteration: 2500 / 5000 [ 50%]  (Warmup)\nChain 1: Iteration: 2501 / 5000 [ 50%]  (Sampling)\nChain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 1: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 1:                0.011 seconds (Sampling)\nChain 1:                0.022 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 5000 [  0%]  (Warmup)\nChain 2: Iteration:  500 / 5000 [ 10%]  (Warmup)\nChain 2: Iteration: 1000 / 5000 [ 20%]  (Warmup)\nChain 2: Iteration: 1500 / 5000 [ 30%]  (Warmup)\nChain 2: Iteration: 2000 / 5000 [ 40%]  (Warmup)\nChain 2: Iteration: 2500 / 5000 [ 50%]  (Warmup)\nChain 2: Iteration: 2501 / 5000 [ 50%]  (Sampling)\nChain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 2: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 2:                0.011 seconds (Sampling)\nChain 2:                0.022 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 5000 [  0%]  (Warmup)\nChain 3: Iteration:  500 / 5000 [ 10%]  (Warmup)\nChain 3: Iteration: 1000 / 5000 [ 20%]  (Warmup)\nChain 3: Iteration: 1500 / 5000 [ 30%]  (Warmup)\nChain 3: Iteration: 2000 / 5000 [ 40%]  (Warmup)\nChain 3: Iteration: 2500 / 5000 [ 50%]  (Warmup)\nChain 3: Iteration: 2501 / 5000 [ 50%]  (Sampling)\nChain 3: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 3: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 3: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 3: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 3: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 3:                0.012 seconds (Sampling)\nChain 3:                0.023 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 5000 [  0%]  (Warmup)\nChain 4: Iteration:  500 / 5000 [ 10%]  (Warmup)\nChain 4: Iteration: 1000 / 5000 [ 20%]  (Warmup)\nChain 4: Iteration: 1500 / 5000 [ 30%]  (Warmup)\nChain 4: Iteration: 2000 / 5000 [ 40%]  (Warmup)\nChain 4: Iteration: 2500 / 5000 [ 50%]  (Warmup)\nChain 4: Iteration: 2501 / 5000 [ 50%]  (Sampling)\nChain 4: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 4: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 4: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 4: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 4: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 4:                0.012 seconds (Sampling)\nChain 4:                0.023 seconds (Total)\nChain 4: \n\n\nWe can extract information from NUTS using\n\nRes <- extract(NUTS, permuted = FALSE)\n\nTo extract the posterior draws, we use\n\nPostDraw <- Res[, , \"theta\"] \n\nTraceplot to check convergence visually\n\ntraceplot(NUTS) \n\n\n\n\nOther summary statistics are\n\n## Rhat: < 1.01\nRhat(PostDraw) \n\n[1] 1.001533\n\n## Effective sample size\nsummary(NUTS)$summary[, \"n_eff\"] \n\n   theta     lp__ \n3882.668 3908.633 \n\n## Number of divergent transitions\n## It occurs if curvature of posterior is large such that it is difficult to explore\nget_num_divergent(NUTS) # Should vanish if control = list(adapt_delta) goes to 1\n\n[1] 0\n\n\nIn fact, you get many summaries from\n\nprint(NUTS)\n\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n        mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\ntheta   0.32    0.00 0.10   0.15   0.25   0.31   0.38   0.52  3883    1\nlp__  -14.26    0.01 0.71 -16.28 -14.42 -13.99 -13.81 -13.76  3909    1\n\nSamples were drawn using NUTS(diag_e) at Tue Feb  4 10:05:30 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nIf we let permuted = TRUE, then we have a vector. Each chain has length 5000 where 2500 will be discarded as burn.in. We have in total 4 chains, then there will be 4 * 2500 left.\n\nPostDraw <- extract(NUTS, permuted = TRUE)$theta \n\nTo make posterior inference. we can use PostDraw directly.\n\n## Posterior mean\nmean(PostDraw)\n\n[1] 0.3182229\n\n## Credible set\nquantile(PostDraw, probs = c(0.025, 0.975)) \n\n     2.5%     97.5% \n0.1500407 0.5181352 \n\n\n\n\n\nLoad package for regression\n\nlibrary(rstanarm) \n\nRead data\n\ndata(penguins, package = \"palmerpenguins\")  \nhead(penguins)  \n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex <fct>, year <int>\n\npenguins <- na.omit(penguins)\n\nTo fit the normal linear model\n\nFit <- stan_glm(bill_length_mm ~ flipper_length_mm + body_mass_g, # our model                 \n                family = gaussian(), # distribution of response                 \n                data = penguins) \n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.48 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.068 seconds (Warm-up)\nChain 1:                0.064 seconds (Sampling)\nChain 1:                0.132 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.6e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.046 seconds (Warm-up)\nChain 2:                0.069 seconds (Sampling)\nChain 2:                0.115 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.4e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.049 seconds (Warm-up)\nChain 3:                0.064 seconds (Sampling)\nChain 3:                0.113 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.047 seconds (Warm-up)\nChain 4:                0.067 seconds (Sampling)\nChain 4:                0.114 seconds (Total)\nChain 4: \n\n\nWe can check the default prior. Note that the package does some internal adjustment to the specified prior.\n\nprior_summary(Fit)\n\nPriors for model 'Fit' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 44, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 44, scale = 14)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [0,0], scale = [2.5,2.5])\n  Adjusted prior:\n    ~ normal(location = [0,0], scale = [0.975,0.017])\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.18)\n------\nSee help('prior_summary.stanreg') for more details\n\n\nIf we want to specify the priors ourselves, it is possible to do so, but not very flexible. For example, if we let autoscale = FALSE, we can switch off the internal adjustment.\n\nFit <- stan_glm(bill_length_mm ~ flipper_length_mm + body_mass_g, # our model                 \n                family = gaussian(), # distribution of response                 \n                prior = normal(location = c(0, 0), scale = c(2.5, 2.5), autoscale = FALSE),                 \n                prior_aux = exponential(rate = 1, autoscale = FALSE), # Prior for sigma    \n                data = penguins) \n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.342 seconds (Warm-up)\nChain 1:                0.42 seconds (Sampling)\nChain 1:                0.762 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.3e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.318 seconds (Warm-up)\nChain 2:                0.444 seconds (Sampling)\nChain 2:                0.762 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 9e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.365 seconds (Warm-up)\nChain 3:                0.374 seconds (Sampling)\nChain 3:                0.739 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.159 seconds (Warm-up)\nChain 4:                0.348 seconds (Sampling)\nChain 4:                0.507 seconds (Total)\nChain 4: \n\n\nHowever, the prior is set on \\(\\sigma\\), not \\(\\sigma^{2}\\). Hence, it is not really possible to use the inverse gamma prior. Further, as the Jeffreys-Lindley Paradox, we cannot use a proper prior to approximate an improper prior. The package only uses the independent prior for the regression coefficients.\nSuppose that we want to tune the above code. I want to change the prior, change the number of chains, and the length.\n\nFit <- stan_glm(bill_length_mm ~ flipper_length_mm + body_mass_g, # our model                 \n                family = gaussian(), # distribution of response                 \n                prior = student_t(df = 3, location = c(0, 0), scale = c(2.5, 2.5), \n                                  autoscale = FALSE),            \n                prior_aux = exponential(rate = 1, autoscale = FALSE), # Prior for sigma           \n                chains = 4, iter = 1e04, warmup = 1e04 / 2, # Pass to rstan\n                data = penguins) \n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.82 seconds (Warm-up)\nChain 1:                1.086 seconds (Sampling)\nChain 1:                2.906 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.897 seconds (Warm-up)\nChain 2:                1.031 seconds (Sampling)\nChain 2:                2.928 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.821 seconds (Warm-up)\nChain 3:                1.09 seconds (Sampling)\nChain 3:                2.911 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.73 seconds (Warm-up)\nChain 4:                1.081 seconds (Sampling)\nChain 4:                2.811 seconds (Total)\nChain 4: \n\n\nAfter fitting the model, we can extract information from the Markov chains.\n\n## Point estimate of coefficients\ncoef(Fit) # Posterior median, not mean\n\n      (Intercept) flipper_length_mm       body_mass_g \n    -3.9379756388      0.2269242564      0.0005569414 \n\n## Posterior Credible interval\nposterior_interval(Fit, prob = 0.95)\n\n                           2.5%       97.5%\n(Intercept)       -1.328282e+01 5.439337466\nflipper_length_mm  1.619856e-01 0.293205240\nbody_mass_g       -5.942682e-04 0.001701734\nsigma              3.836203e+00 4.466880093\n\n## Posterior prediction for each draw after burn-in period \nPredict <- posterior_predict(Fit, newdata = penguins[1 : 5, ])\n\nWe can also extract posterior draws.\n\n## Posterior draws from the Markov chain\nas.matrix(Fit)\n\nYou can conduct posterior predictive check easily.\n\npp_check(Fit) # We can use pp_check(Fit, nreps = 10) to change the replications.\n\n\n\n\nIf we want to do prior predictive check, we use prior_PD = TRUE, then the predictions are only drawn from the prior without using the data\n\nFit <- stan_glm(bill_length_mm ~ flipper_length_mm + body_mass_g, # our model                 \n                family = gaussian(), # distribution of response                 \n                prior = student_t(df = 3, location = c(0, 0), scale = c(2.5, 2.5), \n                                  autoscale = FALSE),            \n                prior_aux = exponential(rate = 1, autoscale = FALSE), # Prior for sigma           \n                prior_PD = TRUE, # prior predictive check\n                data = penguins) \n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 1:                0.037 seconds (Sampling)\nChain 1:                0.071 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.9e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.033 seconds (Warm-up)\nChain 2:                0.025 seconds (Sampling)\nChain 2:                0.058 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.035 seconds (Warm-up)\nChain 3:                0.024 seconds (Sampling)\nChain 3:                0.059 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 4:                0.025 seconds (Sampling)\nChain 4:                0.055 seconds (Total)\nChain 4: \n\npp_check(Fit)\n\n\n\n\nThis is “bad” prior, because predicted Y can be negative quite frequently.\n\nrange(posterior_predict(Fit))\n\n[1] -81291.39  72376.82"
  }
]